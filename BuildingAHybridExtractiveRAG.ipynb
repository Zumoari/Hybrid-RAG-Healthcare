{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRiP+xkbg9Z/nMIaFwxmKL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zumoari/Hybrid-RAG-Healthcare/blob/main/BuildingAHybridExtractiveRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Building a hybrid extractive RAG for large scale healthcare literature\n",
        "This Google Colaboratory provides the code for the RAG created in the above mentioned paper.\n",
        "\n",
        "The code contains the hybrid RAG architecture with the indexing pipeline and querying pipeline.\n",
        "\n",
        "This RAG architecture uses the free Mistral API https://mistral.ai/ as LLM. An API key for the Mistral LLM is **not** provided.\n",
        "\n",
        "For using another LLM please refer to the haystack documentation https://docs.haystack.deepset.ai/docs/generators.\n",
        "\n",
        "\n",
        "Code for document-preprocessing is not included. For further clarification please read the annotated code below."
      ],
      "metadata": {
        "id": "RMYmWIM_QgV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPmRFTQBQWXv"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "\n",
        "%%capture\n",
        "!pip install haystack-ai\n",
        "!pip install \"datasets>=2.6.1\"\n",
        "!pip install \"sentence-transformers>=2.2.0\"\n",
        "!pip install firebase-admin\n",
        "!pip install bitsandbytes\n",
        "!pip install onnxruntime\n",
        "!pip install qdrant-haystack\n",
        "!pip install fastembed-haystack\n",
        "!pip install mistral-haystack\n",
        "# Due to some issues we used these versions for openai and httpx\n",
        "# The issue may be resloved in newer versions\n",
        "# Here is a community blog where the issue is discussed\n",
        "# https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "from haystack import Document\n",
        "from haystack import component\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.dataclasses import Document\n",
        "from typing import List\n",
        "\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.utils import Secret\n",
        "from haystack import Pipeline\n",
        "from haystack_integrations.components.generators.mistral import MistralChatGenerator\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.builders import ChatPromptBuilder\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "\n",
        "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
        "from haystack_integrations.components.retrievers.qdrant import QdrantHybridRetriever\n",
        "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
        "from haystack.document_stores.types import DuplicatePolicy\n",
        "from haystack_integrations.components.embedders.fastembed import (\n",
        "\tFastembedTextEmbedder,\n",
        "\tFastembedDocumentEmbedder,\n",
        "\tFastembedSparseTextEmbedder,\n",
        "\tFastembedSparseDocumentEmbedder\n",
        ")\n",
        "from jinja2 import Template\n",
        "import json"
      ],
      "metadata": {
        "id": "JE9MnxS_Scye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Please preprocess your source documents here\n",
        "# Your documents should be written in a dictionary called \"documents\" and contain the following key-value pairs:\n",
        "# content: contains all the text in the document, which should be used for retrieval\n",
        "# meta: contains the metadata for your documents. It is up to you, what information should be included.\n",
        "# Depending on the information in meta please customize the CustomPostProcessor component\n",
        "# Please refer to the haystack documentation for further information on the document data class https://docs.haystack.deepset.ai/docs/data-classes#document\n",
        "\n",
        "documents = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in documents]\n",
        "len(documents) # Please make sure all documents are contained in this array"
      ],
      "metadata": {
        "id": "zmqsJXbSSuBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Split documents into smaller chunks\n",
        "splitter = DocumentSplitter(\n",
        "    split_by=\"sentence\",\n",
        "    split_length=20,\n",
        "    split_overlap=2\n",
        ")\n",
        "\n",
        "result = splitter.run(documents=documents)\n",
        "\n",
        "docs = result[\"documents\"]\n",
        "print(f\"Processed {len(result['documents'])} document splits.\")"
      ],
      "metadata": {
        "id": "mqI7X9eYSy1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the document store\n",
        "\n",
        "document_store = QdrantDocumentStore(\n",
        "    path=\"/content/qdrant/storage_local\",\n",
        "    embedding_dim=384, # has to be 384 for the hybrid version\n",
        "    recreate_index=True, # False, if a Qdrant Document Store already exists\n",
        "    use_sparse_embeddings=True # For the hybrid RAG\n",
        ")"
      ],
      "metadata": {
        "id": "YwLdLwFxS6Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template\n",
        "\n",
        "# The prompt builder inserts the necessary information later\n",
        "# Please make sure to keep the following parts as is, otherwise the prompt builder is not able to insert the question and the documents\n",
        "# {{ question }}\n",
        "# AND\n",
        "# {% for document in retriever_documents %}\n",
        "#     [ Document :\n",
        "#     ID: {{ document.id }}\n",
        "#     Score: {{ document.score }}\n",
        "#     Meta: {{ document.meta }}\n",
        "#     Content: {{ document.content }}\n",
        "#\n",
        "#     ]\n",
        "#\n",
        "# {% endfor %}\n",
        "# You may change the information in the for loop. Be sure to check the Data Class \"Document\": https://docs.haystack.deepset.ai/docs/data-classes#document\n",
        "\n",
        "template = \"\"\"\n",
        "Question: {{ question }}\n",
        "\n",
        "Answer the question **as clearly and concisely as possible**, only based on the following provided papers:\n",
        "\n",
        "# General Context from Retriever:\n",
        "{% for document in retriever_documents %}\n",
        "    [ Document :\n",
        "    ID: {{ document.id }}\n",
        "    Score: {{ document.score }}\n",
        "    Meta: {{ document.meta }}\n",
        "    Content: {{ document.content }}\n",
        "\n",
        "    ]\n",
        "\n",
        "{% endfor %}\n",
        "\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gGt4ab0sUBpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MistralGenerator = MistralChatGenerator(api_key=Secret.from_token(your_api_key_here), # Please insert your API key or use env-variables\n",
        "                                        model=\"mistral-large-latest\",  # Replace with your desired model name\n",
        "                                        streaming_callback=print_streaming_chunk,  # Optional streaming callback - writes the model output in the console\n",
        "                                        generation_kwargs={\n",
        "                                            \"temperature\": 0.1, # we found that a temperature of 0.1 provides good results. Feel free to change it.\n",
        "                    })"
      ],
      "metadata": {
        "id": "dkMwfD19U22f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Chat Prompt Builder\n",
        "\n",
        "# This component currently does not do much, only inserts the question and the documents in the prompt template\n",
        "# Feel free to add to this component\n",
        "\n",
        "@component\n",
        "class CustomChatPromptBuilder:\n",
        "    def __init__(self, template:List[ChatMessage]):\n",
        "        template = template[0].content\n",
        "        self.template = Template(template.strip())\n",
        "\n",
        "    @component.output_types(prompt=List[ChatMessage])\n",
        "    def run(self, question:str, retriever_documents:List[Document]):\n",
        "\n",
        "        prompt = self.template.render(question=question, retriever_documents=retriever_documents)\n",
        "        return {\"prompt\": [ChatMessage.from_user(content=prompt)]}\n"
      ],
      "metadata": {
        "id": "D6G4TEJzVchN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Post Processor\n",
        "\n",
        "# Creates a string in JSON format of the LLM output and all the sources used to create the answer\n",
        "# Please modify the for-loop depending on your information in your documents\n",
        "\n",
        "@component\n",
        "class CustomPostProcessor:\n",
        "  @component.output_types(text=str)\n",
        "  def run(self, answerLLM:List[ChatMessage], sources:List[Document]):\n",
        "    answer = answerLLM[0].content\n",
        "\n",
        "    source_list = []\n",
        "    for index, eA in enumerate(sources, start=1):\n",
        "        source_list.append({\n",
        "            \"ref\": index,\n",
        "            \"id\": eA.id,\n",
        "            \"score\": eA.score,\n",
        "            \"meta\": eA.meta,\n",
        "            \"content\": eA.content\n",
        "        })\n",
        "\n",
        "    result = {\n",
        "        \"answer\": answer,\n",
        "        \"sources\": source_list\n",
        "    }\n",
        "    return {\"text\": json.dumps(result, ensure_ascii=False, indent=2)}"
      ],
      "metadata": {
        "id": "HuYfGtHxV9QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the indexing pipeline\n",
        "\n",
        "# Creation of the components\n",
        "indexing = Pipeline()\n",
        "indexing.add_component(\"sparse_doc_embedder\", FastembedSparseDocumentEmbedder(model=\"prithvida/Splade_PP_en_v1\"))\n",
        "indexing.add_component(\"dense_doc_embedder\", FastembedDocumentEmbedder(model=\"BAAI/bge-small-en-v1.5\"))\n",
        "indexing.add_component(\"writer\", DocumentWriter(document_store=document_store, policy=DuplicatePolicy.OVERWRITE))\n",
        "\n",
        "# Connection of the components\n",
        "indexing.connect(\"sparse_doc_embedder\", \"dense_doc_embedder\")\n",
        "indexing.connect(\"dense_doc_embedder\", \"writer\")"
      ],
      "metadata": {
        "id": "1GLoAAJJWjlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the indexing pipeline\n",
        "\n",
        "# You only need to run this once for source-document indexing\n",
        "# This may take a while, depending on the amount of document chunks\n",
        "\n",
        "indexing.run({\"sparse_doc_embedder\": {\"documents\": docs}})"
      ],
      "metadata": {
        "id": "dd5shmn4XiDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the querying pipeline\n",
        "\n",
        "# Creation of the components\n",
        "querying = Pipeline()\n",
        "querying.add_component(\"sparse_text_embedder\", FastembedSparseTextEmbedder(model=\"prithvida/Splade_PP_en_v1\"))\n",
        "querying.add_component(\"dense_text_embedder\", FastembedTextEmbedder(\n",
        "\tmodel=\"BAAI/bge-small-en-v1.5\", prefix=\"\")\n",
        "\t)\n",
        "querying.add_component(\"retriever\", QdrantHybridRetriever(document_store=document_store))\n",
        "querying.add_component(\"prompt_builder\", CustomChatPromptBuilder(template=[ChatMessage.from_user(template)]))\n",
        "querying.add_component(\"llm\", MistralGenerator)\n",
        "querying.add_component(\"custom_processor\", CustomPostProcessor())\n",
        "\n",
        "# Connection of the components\n",
        "querying.connect(\"sparse_text_embedder.sparse_embedding\", \"retriever.query_sparse_embedding\")\n",
        "querying.connect(\"dense_text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "querying.connect(\"retriever.documents\", \"prompt_builder.retriever_documents\")\n",
        "querying.connect(\"prompt_builder\", \"llm.messages\")\n",
        "querying.connect(\"retriever.documents\", \"custom_processor.sources\")\n",
        "querying.connect(\"llm.replies\", \"custom_processor.answerLLM\")"
      ],
      "metadata": {
        "id": "T-C0lJTxW-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The input query to the RAG\n",
        "question = \"PLEASE INSERT YOUR QUESTION HERE\""
      ],
      "metadata": {
        "id": "QZ-NuujdXy0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the querying pipeline\n",
        "rag_result = querying.run(\n",
        "    {\"dense_text_embedder\": {\"text\": question},\n",
        "     \"sparse_text_embedder\": {\"text\": question},\n",
        "     \"prompt_builder\": {\"question\": question},\n",
        "     \"retriever\": {\"top_k\": 3} # This is the number of document chunks the LLM recieves. Depending on your chunk-size and the max input size of your LLM you can input more or less.\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "eXz2_mNRXwwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_result) # You can see your result here. Additionally, you could process your results in a JSON file for better readability."
      ],
      "metadata": {
        "id": "4UBILsd6YXwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
